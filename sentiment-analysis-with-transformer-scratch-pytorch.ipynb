{"cells":[{"cell_type":"markdown","metadata":{},"source":["#  **Introduction**"]},{"cell_type":"markdown","metadata":{},"source":["> Recently I've been reading \"**Attention Is All You Need**\" paper aka **Tranformer by Google in 2017**. Later on, **Andrej Karpathy** had explained this paper on simple understandable chuncks. Also I've recently switch to pytorch from tensorflow. So, this notebook is of a begineer trying to implement what he has been learning recently. Pytorch has a inbuilt Transformer on `nn.transformer` but still I've tried my best to implement the decoder only transformer architecture using basic pytorch and this notebook will be guide for someone like me. "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-28T02:51:50.164838Z","iopub.status.busy":"2024-08-28T02:51:50.164464Z","iopub.status.idle":"2024-08-28T02:51:56.549429Z","shell.execute_reply":"2024-08-28T02:51:56.548458Z","shell.execute_reply.started":"2024-08-28T02:51:50.164801Z"},"trusted":true},"outputs":[],"source":["import pandas as pd \n","import os\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{},"source":["#  Directory Reading"]},{"cell_type":"markdown","metadata":{},"source":["# Data Reading\n","> As per the dataset description, each file has 4 column i.e. twitter Id, entity, sentiment and text. I've loaded the both training data and validation data for preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:56.562973Z","iopub.status.busy":"2024-08-28T02:51:56.562573Z","iopub.status.idle":"2024-08-28T02:51:56.881439Z","shell.execute_reply":"2024-08-28T02:51:56.880572Z","shell.execute_reply.started":"2024-08-28T02:51:56.562936Z"},"trusted":true},"outputs":[],"source":["columns_name = [\"t_id\", \"entity\", \"sentiment\", \"text\"]\n","training_data = pd.read_csv(\"dataset/twitter_training.csv\", header=None, names=columns_name, index_col=False)\n","validation_data = pd.read_csv(\"dataset/twitter_validation.csv\", header=None, names=columns_name, index_col=False)"]},{"cell_type":"markdown","metadata":{},"source":["Just tried to peek into the data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:56.883875Z","iopub.status.busy":"2024-08-28T02:51:56.883538Z","iopub.status.idle":"2024-08-28T02:51:56.902790Z","shell.execute_reply":"2024-08-28T02:51:56.901930Z","shell.execute_reply.started":"2024-08-28T02:51:56.883839Z"},"trusted":true},"outputs":[],"source":["training_data.head(), validation_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["> from four columns, only **text** and **sentiment** will be used for the sentiment analysis. text will be the input features and sentiment will be the target. Also I'm printing the shape of trainin_data and validation_data to look into their shape before and after preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:56.906033Z","iopub.status.busy":"2024-08-28T02:51:56.905746Z","iopub.status.idle":"2024-08-28T02:51:56.918768Z","shell.execute_reply":"2024-08-28T02:51:56.917748Z","shell.execute_reply.started":"2024-08-28T02:51:56.906001Z"},"trusted":true},"outputs":[],"source":["training_data = training_data[[\"text\", \"sentiment\"]]\n","validation_data = validation_data[[\"text\", \"sentiment\"]]\n","print(f\"Shape of Training_data:{training_data.shape} | Shape of validation_data:{validation_data.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["> This is a basic text cleaning preprocess. It convert text to lowercase, remove urls, remove hashtag for mentions, remove some punctuation and remove numbers. I think these are irrelevant for sentiment analysis. You can add as per your need."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:56.920358Z","iopub.status.busy":"2024-08-28T02:51:56.920051Z","iopub.status.idle":"2024-08-28T02:51:56.928520Z","shell.execute_reply":"2024-08-28T02:51:56.927645Z","shell.execute_reply.started":"2024-08-28T02:51:56.920328Z"},"trusted":true},"outputs":[],"source":["def clean_tweet(text):\n","    if isinstance(text, str):\n","        text = text.lower()  # Convert to lowercase\n","        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n","        text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n","        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","        text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    else:\n","        text = ''  # Handle non-string inputs like float (NaN) by returning an empty string or handling as needed\n","    \n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:56.930005Z","iopub.status.busy":"2024-08-28T02:51:56.929651Z","iopub.status.idle":"2024-08-28T02:51:58.429866Z","shell.execute_reply":"2024-08-28T02:51:58.429071Z","shell.execute_reply.started":"2024-08-28T02:51:56.929964Z"},"trusted":true},"outputs":[],"source":["training_data[\"text\"] = training_data[\"text\"].apply(clean_tweet)\n","validation_data[\"text\"] = validation_data[\"text\"].apply(clean_tweet)"]},{"cell_type":"markdown","metadata":{},"source":["> Also I've removed stopwords from the text as it is irrelevant to sentiment analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:58.431176Z","iopub.status.busy":"2024-08-28T02:51:58.430892Z","iopub.status.idle":"2024-08-28T02:51:58.760136Z","shell.execute_reply":"2024-08-28T02:51:58.759213Z","shell.execute_reply.started":"2024-08-28T02:51:58.431146Z"},"trusted":true},"outputs":[],"source":["stop_words = set(stopwords.words('english'))\n","print(f\"Length of stopwords:{len(stop_words)}\")\n","\n","def remove_stopwords(text):\n","    return ' '.join([word for word in text.split() if word not in stop_words])\n","\n","training_data[\"text\"] = training_data[\"text\"].apply(remove_stopwords)\n","validation_data[\"text\"] = validation_data[\"text\"].apply(remove_stopwords)\n","print(f\" Shape of Training data: {training_data.shape} | Shape of Validation data: {validation_data.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["> Removing duplicates is relevant to sentiment analysis. Look at the shape of data before and after removing the duplicates."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:58.761896Z","iopub.status.busy":"2024-08-28T02:51:58.761501Z","iopub.status.idle":"2024-08-28T02:51:58.788737Z","shell.execute_reply":"2024-08-28T02:51:58.787598Z","shell.execute_reply.started":"2024-08-28T02:51:58.761852Z"},"trusted":true},"outputs":[],"source":["training_data.drop_duplicates(subset=[\"text\"], inplace=True)\n","validation_data.drop_duplicates(subset=[\"text\"], inplace=True)\n","print(f\" Shape of Training data: {training_data.shape} | Shape of Validation data: {validation_data.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Label encoding\n","I've used LabelEncoder from sklean. You can use other available modules.\n","There are four classes as Sentiment. They are **Positive, Negative, Neutral & Irrelevant**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:58.792633Z","iopub.status.busy":"2024-08-28T02:51:58.792293Z","iopub.status.idle":"2024-08-28T02:51:58.815540Z","shell.execute_reply":"2024-08-28T02:51:58.814516Z","shell.execute_reply.started":"2024-08-28T02:51:58.792597Z"},"trusted":true},"outputs":[],"source":["label_encoder = LabelEncoder()\n","\n","training_data[\"sentiment\"] = label_encoder.fit_transform(training_data[\"sentiment\"])\n","validation_data[\"sentiment\"] = label_encoder.fit_transform(validation_data[\"sentiment\"])\n","label_classes = label_encoder.classes_\n","num_classes = len(label_classes)\n","num_classes"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Dataset from Pytorch\n","> For a cutom dataset, we should override three functions. They are `__init__`, `__len__` and `__getitem__`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:58.817462Z","iopub.status.busy":"2024-08-28T02:51:58.817023Z","iopub.status.idle":"2024-08-28T02:51:58.827457Z","shell.execute_reply":"2024-08-28T02:51:58.826510Z","shell.execute_reply.started":"2024-08-28T02:51:58.817416Z"},"trusted":true},"outputs":[],"source":["class TwitterSentimentDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length):\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        text = self.data.iloc[index][\"text\"]\n","        sentiment = self.data.iloc[index][\"sentiment\"]\n","        \n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length = self.max_length,\n","            padding = 'max_length',\n","            truncation = True,\n","            return_attention_mask = True,\n","            return_tensors = 'pt' \n","        )\n","        label = torch.tensor(sentiment, dtype=torch.long)\n","        \n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask':encoding['attention_mask'].flatten(),\n","            'labels':label\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenization\n","> I've used `BertTokenizer` from huggingface. You can use other as well. Like `tiktoken` (tokenizer of OpenAI), `Sentencepice` by google and many more. In near future I'll use custom tokenizer."]},{"cell_type":"markdown","metadata":{},"source":["> Be careful about `max_length` and `batch_size`. `Max_length` is the maximum length of token to feed into tokenizer and `batch_size` control the number of data that a model interact in each pass per iteration. Having a low batch_size is time expensive and high batch_size is memory expensive and model can't learn the pattern in the data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:58.829374Z","iopub.status.busy":"2024-08-28T02:51:58.828650Z","iopub.status.idle":"2024-08-28T02:51:59.946055Z","shell.execute_reply":"2024-08-28T02:51:59.945144Z","shell.execute_reply.started":"2024-08-28T02:51:58.829330Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","max_length = 128\n","\n","dataset = TwitterSentimentDataset(\n","    training_data,\n","    tokenizer=tokenizer,\n","    max_length=max_length\n",")\n","\n","batch_size = 512\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["> This is the vocabulary size aka vocab_size of the tokenizer. Tokenizer can use 30522 vocab_size (gpt-2 tokenizer had 50,257) to encode and decode text."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:59.947527Z","iopub.status.busy":"2024-08-28T02:51:59.947209Z","iopub.status.idle":"2024-08-28T02:51:59.954270Z","shell.execute_reply":"2024-08-28T02:51:59.953345Z","shell.execute_reply.started":"2024-08-28T02:51:59.947493Z"},"trusted":true},"outputs":[],"source":["vocab_size = tokenizer.vocab_size\n","vocab_size"]},{"cell_type":"markdown","metadata":{},"source":["> These are the hyperparameters for the transformer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:51:59.955470Z","iopub.status.busy":"2024-08-28T02:51:59.955196Z","iopub.status.idle":"2024-08-28T02:52:00.000003Z","shell.execute_reply":"2024-08-28T02:51:59.998932Z","shell.execute_reply.started":"2024-08-28T02:51:59.955441Z"},"trusted":true},"outputs":[],"source":["n_embed = 32\n","block_size = 4\n","dropout = 0.1\n","n_head = 6\n","n_layer = 6\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{},"source":["# Transformer Block -> Self Attention, Multihead Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:52:00.002089Z","iopub.status.busy":"2024-08-28T02:52:00.001444Z","iopub.status.idle":"2024-08-28T02:52:00.019700Z","shell.execute_reply":"2024-08-28T02:52:00.018603Z","shell.execute_reply.started":"2024-08-28T02:52:00.002044Z"},"trusted":true},"outputs":[],"source":["class Self_Attention(nn.Module):\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embed, head_size, bias=False)\n","        self.query = nn.Linear(n_embed, head_size, bias=False)\n","        self.value = nn.Linear(n_embed, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(max_length, max_length)))\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)\n","        q = self.query(x)\n","        v = self.value(x)\n","\n","        wei = torch.matmul(q, k.transpose(-2, -1)) * (k.shape[-1]**-0.5)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  #comment this line for encoder only transformer\n","        wei = nn.Softmax(dim=-1)(wei)\n","        wei = self.dropout(wei)\n","        out = torch.matmul(wei, v)\n","        return out\n","\n","class Multi_Head_Attention(nn.Module):\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Self_Attention(head_size) for _ in range(num_heads)])\n","        self.projection = nn.Linear(head_size * num_heads, n_embed)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([sa(x) for sa in self.heads], dim=-1)\n","        out = self.dropout(self.projection(out))\n","        return out\n","\n","class Feed_Forward_Network(nn.Module):\n","    def __init__(self, n_embed):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.Linear(n_embed, 4 * n_embed),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embed, n_embed),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.layer(x)\n","\n","class Block(nn.Module):\n","    def __init__(self, n_embed, n_head):\n","        super().__init__()\n","        head_size = n_embed // n_head\n","        self.self_attention = Multi_Head_Attention(n_head, head_size)\n","        self.feed_forward = Feed_Forward_Network(n_embed)\n","        self.layer_norm_1 = nn.LayerNorm(n_embed)\n","        self.layer_norm_2 = nn.LayerNorm(n_embed)\n","\n","    def forward(self, x):\n","        x = x + self.self_attention(self.layer_norm_1(x))\n","        x = x + self.feed_forward(self.layer_norm_2(x))\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# SentimentAnalysisTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:52:00.021580Z","iopub.status.busy":"2024-08-28T02:52:00.020921Z","iopub.status.idle":"2024-08-28T02:52:00.033871Z","shell.execute_reply":"2024-08-28T02:52:00.032955Z","shell.execute_reply.started":"2024-08-28T02:52:00.021537Z"},"trusted":true},"outputs":[],"source":["class SentimentAnalysisTransformer(nn.Module):\n","    def __init__(self):  \n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        self.positional_embedding_table = nn.Embedding(max_length, n_embed)\n","        self.block = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embed)\n","        self.lm_head = nn.Linear(n_embed, num_classes)\n","        \n","    def forward(self, x):\n","        B, T = x.shape\n","        tok_emb = self.token_embedding_table(x)  # (B, T, n_embed)\n","        pos_emb = self.positional_embedding_table(torch.arange(T, device=x.device))  # (T, n_embed)\n","        x = tok_emb + pos_emb  # (B, T, n_embed)\n","        x = self.block(x)  # (B, T, n_embed)\n","        x = self.ln_f(x)  # (B, T, n_embed)\n","        logits = self.lm_head(x.mean(dim=1))  # (B, T, vocab_size)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:52:00.035292Z","iopub.status.busy":"2024-08-28T02:52:00.034913Z","iopub.status.idle":"2024-08-28T02:52:00.283159Z","shell.execute_reply":"2024-08-28T02:52:00.282105Z","shell.execute_reply.started":"2024-08-28T02:52:00.035248Z"},"trusted":true},"outputs":[],"source":["model = SentimentAnalysisTransformer()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Loss function and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:52:00.285339Z","iopub.status.busy":"2024-08-28T02:52:00.284583Z","iopub.status.idle":"2024-08-28T02:52:01.260914Z","shell.execute_reply":"2024-08-28T02:52:01.260085Z","shell.execute_reply.started":"2024-08-28T02:52:00.285293Z"},"trusted":true},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T02:52:01.262638Z","iopub.status.busy":"2024-08-28T02:52:01.262179Z","iopub.status.idle":"2024-08-28T04:16:54.572851Z","shell.execute_reply":"2024-08-28T04:16:54.571858Z","shell.execute_reply.started":"2024-08-28T02:52:01.262603Z"},"trusted":true},"outputs":[],"source":["num_epochs= 60\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    \n","    for batch in dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        \n","        y_hat = model(input_ids)\n","        loss = loss_fn(y_hat, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()        \n","    epoch_loss = running_loss / len(dataloader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Validation Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:16:54.574393Z","iopub.status.busy":"2024-08-28T04:16:54.574057Z","iopub.status.idle":"2024-08-28T04:16:54.579045Z","shell.execute_reply":"2024-08-28T04:16:54.578148Z","shell.execute_reply.started":"2024-08-28T04:16:54.574360Z"},"trusted":true},"outputs":[],"source":["validation_dataset = TwitterSentimentDataset(validation_data, tokenizer=tokenizer, max_length=max_length)\n","validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:16:54.580484Z","iopub.status.busy":"2024-08-28T04:16:54.580211Z","iopub.status.idle":"2024-08-28T04:16:55.724639Z","shell.execute_reply":"2024-08-28T04:16:55.723811Z","shell.execute_reply.started":"2024-08-28T04:16:54.580455Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","all_pred = []\n","all_labels = []\n","\n","with torch.inference_mode():\n","    for batch in validation_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","    outputs = model(input_ids)\n","    _, preds = torch.max(outputs, dim=1)\n","\n","    all_pred.extend(preds.cpu().numpy())\n","    all_labels.extend(labels.cpu().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["# Accuracy Calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:16:55.725919Z","iopub.status.busy":"2024-08-28T04:16:55.725649Z","iopub.status.idle":"2024-08-28T04:16:55.733598Z","shell.execute_reply":"2024-08-28T04:16:55.732720Z","shell.execute_reply.started":"2024-08-28T04:16:55.725890Z"},"trusted":true},"outputs":[],"source":["accuracy = accuracy_score(all_labels, all_pred)\n","print(f\"validation accuracy is:{accuracy}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1520310,"sourceId":2510329,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
