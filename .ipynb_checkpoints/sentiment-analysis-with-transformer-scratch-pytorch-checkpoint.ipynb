{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recently I've been reading \"**Attention Is All You Need**\" paper aka **Tranformer by Google in 2017**. Later on, **Andrej Karpathy** had explained this paper on simple understandable chuncks. Also I've recently switch to pytorch from tensorflow. So, this notebook is of a begineer trying to implement what he has been learning recently. Pytorch has a inbuilt Transformer on `nn.transformer` but still I've tried my best to implement the decoder only transformer architecture using basic pytorch and this notebook will be guide for someone like me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rodolfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\rodolfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Directory Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading\n",
    "> As per the dataset description, each file has 4 column i.e. twitter Id, entity, sentiment and text. I've loaded the both training data and validation data for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = [\"t_id\", \"entity\", \"sentiment\", \"text\"]\n",
    "training_data = pd.read_csv(\"dataset/twitter_training.csv\", header=None, names=columns_name, index_col=False)\n",
    "validation_data = pd.read_csv(\"dataset/twitter_validation.csv\", header=None, names=columns_name, index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just tried to peek into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   t_id       entity sentiment  \\\n",
       " 0  2401  Borderlands  Positive   \n",
       " 1  2401  Borderlands  Positive   \n",
       " 2  2401  Borderlands  Positive   \n",
       " 3  2401  Borderlands  Positive   \n",
       " 4  2401  Borderlands  Positive   \n",
       " \n",
       "                                                 text  \n",
       " 0  im getting on borderlands and i will murder yo...  \n",
       " 1  I am coming to the borders and I will kill you...  \n",
       " 2  im getting on borderlands and i will kill you ...  \n",
       " 3  im coming on borderlands and i will murder you...  \n",
       " 4  im getting on borderlands 2 and i will murder ...  ,\n",
       "    t_id     entity   sentiment  \\\n",
       " 0  3364   Facebook  Irrelevant   \n",
       " 1   352     Amazon     Neutral   \n",
       " 2  8312  Microsoft    Negative   \n",
       " 3  4371      CS-GO    Negative   \n",
       " 4  4433     Google     Neutral   \n",
       " \n",
       "                                                 text  \n",
       " 0  I mentioned on Facebook that I was struggling ...  \n",
       " 1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
       " 2  @Microsoft Why do I pay for WORD when it funct...  \n",
       " 3  CSGO matchmaking is so full of closet hacking,...  \n",
       " 4  Now the President is slapping Americans in the...  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head(), validation_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> from four columns, only **text** and **sentiment** will be used for the sentiment analysis. text will be the input features and sentiment will be the target. Also I'm printing the shape of trainin_data and validation_data to look into their shape before and after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training_data:(74682, 2) | Shape of validation_data:(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = training_data[[\"text\", \"sentiment\"]]\n",
    "validation_data = validation_data[[\"text\", \"sentiment\"]]\n",
    "print(f\"Shape of Training_data:{training_data.shape} | Shape of validation_data:{validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a basic text cleaning preprocess. It convert text to lowercase, remove urls, remove hashtag for mentions, remove some punctuation and remove numbers. I think these are irrelevant for sentiment analysis. You can add as per your need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "        text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    else:\n",
    "        text = ''  # Handle non-string inputs like float (NaN) by returning an empty string or handling as needed\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"text\"] = training_data[\"text\"].apply(clean_tweet)\n",
    "validation_data[\"text\"] = validation_data[\"text\"].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Also I've removed stopwords from the text as it is irrelevant to sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of stopwords:179\n",
      " Shape of Training data: (74682, 2) | Shape of Validation data: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Length of stopwords:{len(stop_words)}\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "training_data[\"text\"] = training_data[\"text\"].apply(remove_stopwords)\n",
    "validation_data[\"text\"] = validation_data[\"text\"].apply(remove_stopwords)\n",
    "print(f\" Shape of Training data: {training_data.shape} | Shape of Validation data: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Removing duplicates is relevant to sentiment analysis. Look at the shape of data before and after removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of Training data: (62166, 2) | Shape of Validation data: (997, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data.drop_duplicates(subset=[\"text\"], inplace=True)\n",
    "validation_data.drop_duplicates(subset=[\"text\"], inplace=True)\n",
    "print(f\" Shape of Training data: {training_data.shape} | Shape of Validation data: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding\n",
    "I've used LabelEncoder from sklean. You can use other available modules.\n",
    "There are four classes as Sentiment. They are **Positive, Negative, Neutral & Irrelevant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "training_data[\"sentiment\"] = label_encoder.fit_transform(training_data[\"sentiment\"])\n",
    "validation_data[\"sentiment\"] = label_encoder.fit_transform(validation_data[\"sentiment\"])\n",
    "label_classes = label_encoder.classes_\n",
    "num_classes = len(label_classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset from Pytorch\n",
    "> For a cutom dataset, we should override three functions. They are `__init__`, `__len__` and `__getitem__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index][\"text\"]\n",
    "        sentiment = self.data.iloc[index][\"sentiment\"]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt' \n",
    "        )\n",
    "        label = torch.tensor(sentiment, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask':encoding['attention_mask'].flatten(),\n",
    "            'labels':label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "> I've used `BertTokenizer` from huggingface. You can use other as well. Like `tiktoken` (tokenizer of OpenAI), `Sentencepice` by google and many more. In near future I'll use custom tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Be careful about `max_length` and `batch_size`. `Max_length` is the maximum length of token to feed into tokenizer and `batch_size` control the number of data that a model interact in each pass per iteration. Having a low batch_size is time expensive and high batch_size is memory expensive and model can't learn the pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodolfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 128\n",
    "\n",
    "dataset = TwitterSentimentDataset(\n",
    "    training_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "batch_size = 512\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is the vocabulary size aka vocab_size of the tokenizer. Tokenizer can use 30522 vocab_size (gpt-2 tokenizer had 50,257) to encode and decode text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These are the hyperparameters for the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed = 32\n",
    "block_size = 4\n",
    "dropout = 0.1\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block -> Self Attention, Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_length, max_length)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = torch.matmul(q, k.transpose(-2, -1)) * (k.shape[-1]**-0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  #comment this line for encoder only transformer\n",
    "        wei = nn.Softmax(dim=-1)(wei)\n",
    "        wei = self.dropout(wei)\n",
    "        out = torch.matmul(wei, v)\n",
    "        return out\n",
    "\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Self_Attention(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(head_size * num_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([sa(x) for sa in self.heads], dim=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out\n",
    "\n",
    "class Feed_Forward_Network(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attention = Multi_Head_Attention(n_head, head_size)\n",
    "        self.feed_forward = Feed_Forward_Network(n_embed)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentimentAnalysisTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisTransformer(nn.Module):\n",
    "    def __init__(self):  \n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(max_length, n_embed)\n",
    "        self.block = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding_table(x)  # (B, T, n_embed)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T, device=x.device))  # (T, n_embed)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embed)\n",
    "        x = self.block(x)  # (B, T, n_embed)\n",
    "        x = self.ln_f(x)  # (B, T, n_embed)\n",
    "        logits = self.lm_head(x.mean(dim=1))  # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisTransformer(\n",
       "  (token_embedding_table): Embedding(30522, 32)\n",
       "  (positional_embedding_table): Embedding(128, 32)\n",
       "  (block): Sequential(\n",
       "    (0): Block(\n",
       "      (self_attention): Multi_Head_Attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Self_Attention(\n",
       "            (key): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=30, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Feed_Forward_Network(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (self_attention): Multi_Head_Attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Self_Attention(\n",
       "            (key): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=30, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Feed_Forward_Network(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (self_attention): Multi_Head_Attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Self_Attention(\n",
       "            (key): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=30, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Feed_Forward_Network(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (self_attention): Multi_Head_Attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Self_Attention(\n",
       "            (key): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=30, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Feed_Forward_Network(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (self_attention): Multi_Head_Attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Self_Attention(\n",
       "            (key): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=30, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Feed_Forward_Network(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (self_attention): Multi_Head_Attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Self_Attention(\n",
       "            (key): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=5, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Linear(in_features=30, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): Feed_Forward_Network(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentimentAnalysisTransformer()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Loss: 1.3426\n",
      "Epoch 2/60, Loss: 1.2323\n",
      "Epoch 3/60, Loss: 1.1179\n",
      "Epoch 4/60, Loss: 1.0277\n",
      "Epoch 5/60, Loss: 0.9509\n",
      "Epoch 6/60, Loss: 0.8700\n",
      "Epoch 7/60, Loss: 0.7985\n",
      "Epoch 8/60, Loss: 0.7346\n",
      "Epoch 9/60, Loss: 0.6737\n",
      "Epoch 10/60, Loss: 0.6174\n",
      "Epoch 11/60, Loss: 0.5663\n",
      "Epoch 12/60, Loss: 0.5225\n",
      "Epoch 13/60, Loss: 0.4809\n",
      "Epoch 14/60, Loss: 0.4408\n",
      "Epoch 15/60, Loss: 0.4087\n",
      "Epoch 16/60, Loss: 0.3783\n",
      "Epoch 17/60, Loss: 0.3523\n",
      "Epoch 18/60, Loss: 0.3237\n",
      "Epoch 19/60, Loss: 0.3040\n",
      "Epoch 20/60, Loss: 0.2851\n",
      "Epoch 21/60, Loss: 0.2665\n",
      "Epoch 22/60, Loss: 0.2471\n",
      "Epoch 23/60, Loss: 0.2409\n",
      "Epoch 24/60, Loss: 0.2188\n",
      "Epoch 25/60, Loss: 0.2077\n",
      "Epoch 26/60, Loss: 0.1935\n",
      "Epoch 27/60, Loss: 0.1840\n",
      "Epoch 28/60, Loss: 0.1793\n",
      "Epoch 29/60, Loss: 0.1658\n",
      "Epoch 30/60, Loss: 0.1576\n",
      "Epoch 31/60, Loss: 0.1539\n",
      "Epoch 32/60, Loss: 0.1439\n",
      "Epoch 33/60, Loss: 0.1413\n",
      "Epoch 34/60, Loss: 0.1341\n",
      "Epoch 35/60, Loss: 0.1298\n",
      "Epoch 36/60, Loss: 0.1200\n",
      "Epoch 37/60, Loss: 0.1179\n",
      "Epoch 38/60, Loss: 0.1099\n",
      "Epoch 39/60, Loss: 0.1065\n",
      "Epoch 40/60, Loss: 0.1043\n",
      "Epoch 41/60, Loss: 0.1007\n",
      "Epoch 42/60, Loss: 0.0971\n",
      "Epoch 43/60, Loss: 0.0969\n",
      "Epoch 44/60, Loss: 0.0918\n",
      "Epoch 45/60, Loss: 0.0901\n",
      "Epoch 46/60, Loss: 0.0852\n",
      "Epoch 47/60, Loss: 0.0844\n",
      "Epoch 48/60, Loss: 0.0817\n",
      "Epoch 49/60, Loss: 0.0743\n",
      "Epoch 50/60, Loss: 0.0740\n",
      "Epoch 51/60, Loss: 0.0715\n",
      "Epoch 52/60, Loss: 0.0705\n",
      "Epoch 53/60, Loss: 0.0736\n",
      "Epoch 54/60, Loss: 0.0659\n",
      "Epoch 55/60, Loss: 0.0680\n",
      "Epoch 56/60, Loss: 0.0612\n",
      "Epoch 57/60, Loss: 0.0610\n",
      "Epoch 58/60, Loss: 0.0638\n",
      "Epoch 59/60, Loss: 0.0571\n",
      "Epoch 60/60, Loss: 0.0558\n"
     ]
    }
   ],
   "source": [
    "num_epochs= 60\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        y_hat = model(input_ids)\n",
    "        loss = loss_fn(y_hat, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()        \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = TwitterSentimentDataset(validation_data, tokenizer=tokenizer, max_length=max_length)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_pred = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in validation_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "    all_pred.extend(preds.cpu().numpy())\n",
    "    all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy is:0.931958762886598\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(all_labels, all_pred)\n",
    "print(f\"validation accuracy is:{accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1520310,
     "sourceId": 2510329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
